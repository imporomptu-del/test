{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64e473d6-6c62-4412-9d90-b11e4722ec8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Using device: cuda\n",
      "final text_encoder_type: bert-base-uncased\n",
      "üì∑ Found 11033 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100 images from 0 to 100 finished, ‚è± Batch time: 40.56s; Total time: 40.56s\n",
      " 100 images from 100 to 200 finished, ‚è± Batch time: 20.81s; Total time: 61.37s\n",
      " 100 images from 200 to 300 finished, ‚è± Batch time: 22.69s; Total time: 84.06s\n",
      " 100 images from 300 to 400 finished, ‚è± Batch time: 21.32s; Total time: 105.38s\n",
      " 100 images from 400 to 500 finished, ‚è± Batch time: 22.71s; Total time: 128.08s\n",
      " 100 images from 500 to 600 finished, ‚è± Batch time: 22.50s; Total time: 150.59s\n",
      " 100 images from 600 to 700 finished, ‚è± Batch time: 25.60s; Total time: 176.19s\n",
      " 100 images from 700 to 800 finished, ‚è± Batch time: 27.09s; Total time: 203.28s\n",
      " 100 images from 800 to 900 finished, ‚è± Batch time: 28.03s; Total time: 231.31s\n",
      " 100 images from 900 to 1000 finished, ‚è± Batch time: 28.90s; Total time: 260.21s\n",
      " 100 images from 1000 to 1100 finished, ‚è± Batch time: 27.03s; Total time: 287.24s\n",
      " 100 images from 1100 to 1200 finished, ‚è± Batch time: 25.29s; Total time: 312.53s\n",
      " 100 images from 1200 to 1300 finished, ‚è± Batch time: 26.93s; Total time: 339.45s\n",
      " 100 images from 1300 to 1400 finished, ‚è± Batch time: 28.30s; Total time: 367.75s\n",
      " 100 images from 1400 to 1500 finished, ‚è± Batch time: 24.24s; Total time: 391.99s\n",
      " 100 images from 1500 to 1600 finished, ‚è± Batch time: 43.66s; Total time: 435.65s\n",
      " 100 images from 1600 to 1700 finished, ‚è± Batch time: 32.16s; Total time: 467.80s\n",
      " 100 images from 1700 to 1800 finished, ‚è± Batch time: 27.59s; Total time: 495.40s\n",
      " 100 images from 1800 to 1900 finished, ‚è± Batch time: 49.08s; Total time: 544.48s\n",
      " 100 images from 1900 to 2000 finished, ‚è± Batch time: 53.98s; Total time: 598.46s\n",
      " 100 images from 2000 to 2100 finished, ‚è± Batch time: 30.90s; Total time: 629.37s\n",
      " 100 images from 2100 to 2200 finished, ‚è± Batch time: 41.76s; Total time: 671.13s\n",
      " 100 images from 2200 to 2300 finished, ‚è± Batch time: 25.89s; Total time: 697.02s\n",
      " 100 images from 2300 to 2400 finished, ‚è± Batch time: 41.22s; Total time: 738.25s\n",
      " 100 images from 2400 to 2500 finished, ‚è± Batch time: 22.92s; Total time: 761.17s\n",
      " 100 images from 2500 to 2600 finished, ‚è± Batch time: 37.24s; Total time: 798.41s\n",
      " 100 images from 2600 to 2700 finished, ‚è± Batch time: 37.43s; Total time: 835.83s\n",
      " 100 images from 2700 to 2800 finished, ‚è± Batch time: 29.51s; Total time: 865.34s\n",
      " 100 images from 2800 to 2900 finished, ‚è± Batch time: 22.78s; Total time: 888.12s\n",
      " 100 images from 2900 to 3000 finished, ‚è± Batch time: 27.80s; Total time: 915.92s\n",
      " 100 images from 3000 to 3100 finished, ‚è± Batch time: 24.08s; Total time: 940.00s\n",
      " 100 images from 3100 to 3200 finished, ‚è± Batch time: 25.25s; Total time: 965.25s\n",
      " 100 images from 3200 to 3300 finished, ‚è± Batch time: 37.32s; Total time: 1002.57s\n",
      " 100 images from 3300 to 3400 finished, ‚è± Batch time: 115.56s; Total time: 1118.13s\n",
      " 100 images from 3400 to 3500 finished, ‚è± Batch time: 45.37s; Total time: 1163.49s\n",
      " 100 images from 3500 to 3600 finished, ‚è± Batch time: 22.41s; Total time: 1185.91s\n",
      " 100 images from 3600 to 3700 finished, ‚è± Batch time: 36.91s; Total time: 1222.82s\n",
      " 100 images from 3700 to 3800 finished, ‚è± Batch time: 25.93s; Total time: 1248.75s\n",
      " 100 images from 3800 to 3900 finished, ‚è± Batch time: 29.35s; Total time: 1278.10s\n",
      " 100 images from 3900 to 4000 finished, ‚è± Batch time: 28.31s; Total time: 1306.41s\n",
      " 100 images from 4000 to 4100 finished, ‚è± Batch time: 24.67s; Total time: 1331.08s\n",
      " 100 images from 4100 to 4200 finished, ‚è± Batch time: 27.01s; Total time: 1358.09s\n",
      " 100 images from 4200 to 4300 finished, ‚è± Batch time: 33.83s; Total time: 1391.92s\n",
      " 100 images from 4300 to 4400 finished, ‚è± Batch time: 28.70s; Total time: 1420.62s\n",
      " 100 images from 4400 to 4500 finished, ‚è± Batch time: 22.60s; Total time: 1443.22s\n",
      " 100 images from 4500 to 4600 finished, ‚è± Batch time: 21.65s; Total time: 1464.87s\n",
      " 100 images from 4600 to 4700 finished, ‚è± Batch time: 24.19s; Total time: 1489.06s\n",
      " 100 images from 4700 to 4800 finished, ‚è± Batch time: 24.54s; Total time: 1513.60s\n",
      " 100 images from 4800 to 4900 finished, ‚è± Batch time: 52.77s; Total time: 1566.37s\n",
      " 100 images from 4900 to 5000 finished, ‚è± Batch time: 44.89s; Total time: 1611.26s\n",
      " 100 images from 5000 to 5100 finished, ‚è± Batch time: 40.53s; Total time: 1651.79s\n",
      " 100 images from 5100 to 5200 finished, ‚è± Batch time: 20.89s; Total time: 1672.68s\n",
      " 100 images from 5200 to 5300 finished, ‚è± Batch time: 26.48s; Total time: 1699.16s\n",
      " 100 images from 5300 to 5400 finished, ‚è± Batch time: 27.97s; Total time: 1727.13s\n",
      " 100 images from 5400 to 5500 finished, ‚è± Batch time: 29.63s; Total time: 1756.76s\n",
      " 100 images from 5500 to 5600 finished, ‚è± Batch time: 23.82s; Total time: 1780.58s\n",
      " 100 images from 5600 to 5700 finished, ‚è± Batch time: 22.22s; Total time: 1802.80s\n",
      " 100 images from 5700 to 5800 finished, ‚è± Batch time: 28.10s; Total time: 1830.90s\n",
      " 100 images from 5800 to 5900 finished, ‚è± Batch time: 21.52s; Total time: 1852.43s\n",
      "[/mnt/d/whales/happy-whale-and-dolphin/train_images/e6f3ae2a83e434.jpg] still cannot cannot find a box ...\n",
      " 100 images from 5900 to 6000 finished, ‚è± Batch time: 27.96s; Total time: 1880.38s\n",
      " 100 images from 6000 to 6100 finished, ‚è± Batch time: 26.58s; Total time: 1906.96s\n",
      " 100 images from 6100 to 6200 finished, ‚è± Batch time: 29.47s; Total time: 1936.43s\n",
      " 100 images from 6200 to 6300 finished, ‚è± Batch time: 39.81s; Total time: 1976.25s\n",
      " 100 images from 6300 to 6400 finished, ‚è± Batch time: 23.17s; Total time: 1999.42s\n",
      " 100 images from 6400 to 6500 finished, ‚è± Batch time: 25.40s; Total time: 2024.82s\n",
      " 100 images from 6500 to 6600 finished, ‚è± Batch time: 49.78s; Total time: 2074.60s\n",
      " 100 images from 6600 to 6700 finished, ‚è± Batch time: 20.74s; Total time: 2095.34s\n",
      " 100 images from 6700 to 6800 finished, ‚è± Batch time: 31.78s; Total time: 2127.12s\n",
      " 100 images from 6800 to 6900 finished, ‚è± Batch time: 138.88s; Total time: 2266.00s\n",
      " 100 images from 6900 to 7000 finished, ‚è± Batch time: 24.15s; Total time: 2290.14s\n",
      " 100 images from 7000 to 7100 finished, ‚è± Batch time: 59.89s; Total time: 2350.04s\n",
      " 100 images from 7100 to 7200 finished, ‚è± Batch time: 36.34s; Total time: 2386.38s\n",
      " 100 images from 7200 to 7300 finished, ‚è± Batch time: 26.79s; Total time: 2413.17s\n",
      " 100 images from 7300 to 7400 finished, ‚è± Batch time: 33.33s; Total time: 2446.50s\n",
      " 100 images from 7400 to 7500 finished, ‚è± Batch time: 30.64s; Total time: 2477.14s\n",
      " 100 images from 7500 to 7600 finished, ‚è± Batch time: 40.41s; Total time: 2517.55s\n",
      " 100 images from 7600 to 7700 finished, ‚è± Batch time: 38.27s; Total time: 2555.81s\n",
      " 100 images from 7700 to 7800 finished, ‚è± Batch time: 140.81s; Total time: 2696.63s\n",
      " 100 images from 7800 to 7900 finished, ‚è± Batch time: 56.73s; Total time: 2753.36s\n",
      " 100 images from 7900 to 8000 finished, ‚è± Batch time: 52.02s; Total time: 2805.39s\n",
      " 100 images from 8000 to 8100 finished, ‚è± Batch time: 78.94s; Total time: 2884.33s\n",
      " 100 images from 8100 to 8200 finished, ‚è± Batch time: 29.44s; Total time: 2913.77s\n",
      " 100 images from 8200 to 8300 finished, ‚è± Batch time: 20.85s; Total time: 2934.62s\n",
      " 100 images from 8300 to 8400 finished, ‚è± Batch time: 88.28s; Total time: 3022.90s\n",
      " 100 images from 8400 to 8500 finished, ‚è± Batch time: 62.65s; Total time: 3085.55s\n",
      " 100 images from 8500 to 8600 finished, ‚è± Batch time: 37.06s; Total time: 3122.61s\n",
      " 100 images from 8600 to 8700 finished, ‚è± Batch time: 51.26s; Total time: 3173.87s\n",
      " 100 images from 8700 to 8800 finished, ‚è± Batch time: 122.80s; Total time: 3296.67s\n",
      " 100 images from 8800 to 8900 finished, ‚è± Batch time: 81.84s; Total time: 3378.52s\n",
      " 100 images from 8900 to 9000 finished, ‚è± Batch time: 31.95s; Total time: 3410.46s\n",
      " 100 images from 9000 to 9100 finished, ‚è± Batch time: 59.83s; Total time: 3470.30s\n",
      " 100 images from 9100 to 9200 finished, ‚è± Batch time: 31.71s; Total time: 3502.00s\n",
      " 100 images from 9200 to 9300 finished, ‚è± Batch time: 41.71s; Total time: 3543.71s\n",
      " 100 images from 9300 to 9400 finished, ‚è± Batch time: 76.09s; Total time: 3619.80s\n",
      " 100 images from 9400 to 9500 finished, ‚è± Batch time: 34.77s; Total time: 3654.57s\n",
      " 100 images from 9500 to 9600 finished, ‚è± Batch time: 35.72s; Total time: 3690.29s\n",
      " 100 images from 9600 to 9700 finished, ‚è± Batch time: 44.78s; Total time: 3735.10s\n",
      " 100 images from 9700 to 9800 finished, ‚è± Batch time: 21.56s; Total time: 3756.66s\n",
      " 100 images from 9800 to 9900 finished, ‚è± Batch time: 23.20s; Total time: 3779.86s\n",
      " 100 images from 9900 to 10000 finished, ‚è± Batch time: 39.84s; Total time: 3819.70s\n",
      " 100 images from 10000 to 10100 finished, ‚è± Batch time: 49.57s; Total time: 3869.27s\n",
      " 100 images from 10100 to 10200 finished, ‚è± Batch time: 40.06s; Total time: 3909.33s\n",
      " 100 images from 10200 to 10300 finished, ‚è± Batch time: 42.93s; Total time: 3952.26s\n",
      " 100 images from 10300 to 10400 finished, ‚è± Batch time: 34.70s; Total time: 3986.97s\n",
      " 100 images from 10400 to 10500 finished, ‚è± Batch time: 67.67s; Total time: 4054.64s\n",
      " 100 images from 10500 to 10600 finished, ‚è± Batch time: 22.96s; Total time: 4077.60s\n",
      " 100 images from 10600 to 10700 finished, ‚è± Batch time: 32.69s; Total time: 4110.28s\n",
      " 100 images from 10700 to 10800 finished, ‚è± Batch time: 32.75s; Total time: 4143.04s\n",
      " 100 images from 10800 to 10900 finished, ‚è± Batch time: 20.87s; Total time: 4163.90s\n",
      " 100 images from 10900 to 11000 finished, ‚è± Batch time: 26.42s; Total time: 4190.32s\n",
      " 100 images from 11000 to 11100 finished, ‚è± Batch time: 21.36s; Total time: 4211.68s\n"
     ]
    }
   ],
   "source": [
    "import os, time, cv2, queue, threading, glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import supervision as sv\n",
    "from groundingdino.util.inference import load_model, load_image, predict, annotate\n",
    "\n",
    "# ---------- Config ----------\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "config_path = \"GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\"\n",
    "weights_path = \"GroundingDINO/weights/groundingdino_swint_ogc.pth\"\n",
    "image_folder = \"/mnt/d/whales/happy-whale-and-dolphin/train_images/\"\n",
    "OUTPUT_FOLDER = \"/mnt/d/whales/processed/4\"\n",
    "text_prompt = \"whale\"\n",
    "MAX_IMAGES = 100           # test subset (change to 50000 later)\n",
    "MAX_WORKERS = 4            # loader threads\n",
    "BUFFER_SIZE = 64           # how many decoded images to keep ready\n",
    "BOX_THRESHOLD = 0.4\n",
    "TEXT_THRESHOLD = 0.25\n",
    "RESIZE_TO = (640, 640)\n",
    "CLASS_ID = 5 # whale = 5 for now\n",
    "val_txt = \"dev/whales/yolo/data/train.txt\"\n",
    "\n",
    "def boxes_normalized_to_pixels(boxes, src, path):\n",
    "    h, w, _ = src.shape\n",
    "    # print(path)\n",
    "    ret = []\n",
    "    for box in boxes:\n",
    "        x, y, bw, bh = box\n",
    "        x1 = int((x - bw/2) * w)\n",
    "        y1 = int((y - bh/2) * h)\n",
    "        x2 = int((x + bw/2) * w)\n",
    "        y2 = int((y + bh/2) * h)\n",
    "        x1, y1, x2, y2 = map(float, (x1, y1, x2, y2))\n",
    "        ret.append([x1, y1, x2, y2])\n",
    "    ret = torch.tensor(ret)\n",
    "    return ret\n",
    "            \n",
    "\n",
    "# 1. Async loader with resize\n",
    "def prefetch_loader(paths, loader_func, resize_to=None, max_workers=4, buffer_size=64):\n",
    "    q = queue.Queue(maxsize=buffer_size)\n",
    "    sentinel = object()\n",
    "    it = iter(paths)\n",
    "    lock = threading.Lock()\n",
    "\n",
    "    def worker():\n",
    "        while True:\n",
    "            with lock:\n",
    "                try:\n",
    "                    p = next(it)\n",
    "                except StopIteration:\n",
    "                    break\n",
    "            t0 = time.time()\n",
    "            try:\n",
    "                src, trans = loader_func(p)\n",
    "\n",
    "                if resize_to is not None:\n",
    "                    w, h = resize_to\n",
    "                    src = cv2.resize(src, (w, h), interpolation=cv2.INTER_LINEAR)\n",
    "                    trans = torch.nn.functional.interpolate(\n",
    "                        trans.unsqueeze(0), size=(h, w),\n",
    "                        mode=\"bilinear\", align_corners=False\n",
    "                    ).squeeze(0)\n",
    "\n",
    "                load_t = time.time() - t0\n",
    "                q.put((p, src, trans, load_t))\n",
    "            except Exception as e:\n",
    "                q.put((p, e, None, 0))\n",
    "        q.put(sentinel)\n",
    "\n",
    "    threads = [threading.Thread(target=worker, daemon=True) for _ in range(max_workers)]\n",
    "    for t in threads:\n",
    "        t.start()\n",
    "\n",
    "    finished = 0\n",
    "    while finished < max_workers:\n",
    "        item = q.get()\n",
    "        if item is sentinel:\n",
    "            finished += 1\n",
    "            continue\n",
    "        yield item\n",
    "\n",
    "\n",
    "# 2. Inference\n",
    "def run_inference(model, batch_tensors, batch_paths, batch_src):\n",
    "    import torchvision\n",
    "\n",
    "    t0 = time.time()\n",
    "    trans = batch_tensors[0]\n",
    "    src = batch_src[0]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            boxes, logits, phrases = predict(\n",
    "                model=model,\n",
    "                image=trans,\n",
    "                caption=text_prompt,\n",
    "                box_threshold=BOX_THRESHOLD,\n",
    "                text_threshold=TEXT_THRESHOLD\n",
    "            )\n",
    "\n",
    "    box_threshold = 0.36\n",
    "    while len(boxes) == 0 and box_threshold > 0.29:\n",
    "        # print(f\"[] cannot find box, trying lower confidence ...\")\n",
    "        box_threshold -= 0.01\n",
    "        boxes, logits, phrases = predict(\n",
    "            model=model,\n",
    "            image=trans,\n",
    "            caption=text_prompt,\n",
    "            box_threshold=box_threshold,\n",
    "            text_threshold=0.25\n",
    "        )\n",
    "        \n",
    "    if len(boxes) == 0:  \n",
    "        print(f\"[{batch_paths[0]}] still cannot cannot find a box ...\")\n",
    "\n",
    "        \n",
    "    # Optional NMS (keep)\n",
    "    if len(boxes) > 1:\n",
    "        pixel_boxes = boxes_normalized_to_pixels(boxes, src, batch_paths[0])\n",
    "        keep = torchvision.ops.nms(pixel_boxes, logits, iou_threshold=0.5)\n",
    "        boxes = boxes[keep]\n",
    "        logits = logits[keep]\n",
    "        phrases = [phrases[i] for i in keep]\n",
    "\n",
    "    if DEVICE == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "    inf_t = time.time() - t0\n",
    "    return boxes, logits, phrases, inf_t\n",
    "\n",
    "\n",
    "# 3. Annotate + save\n",
    "def annotate_and_save(image_path, src, outputs):\n",
    "    import cv2, os\n",
    "\n",
    "    boxes, logits, phrases, _ = outputs\n",
    "    annotated = annotate(src, boxes, logits, phrases)\n",
    "\n",
    "    # Save YOLO labels\n",
    "    h, w, _ = src.shape\n",
    "    base = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    with open(os.path.join(OUTPUT_FOLDER, \"labels\", f\"{base}.txt\"), \"w\") as f:\n",
    "        for box in boxes:\n",
    "            x_center, y_center, box_w, box_h = box\n",
    "            f.write(f\"{CLASS_ID} {x_center:.6f} {y_center:.6f} {box_w:.6f} {box_h:.6f}\\n\")\n",
    "\n",
    "    cv2.imwrite(os.path.join(OUTPUT_FOLDER, \"annotated\", f\"{base}.jpg\"), cv2.cvtColor(annotated, cv2.COLOR_RGB2BGR))\n",
    "    cv2.imwrite(os.path.join(OUTPUT_FOLDER, \"images\", f\"{base}.jpg\"), cv2.cvtColor(src, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "\n",
    "# 4. Dataset loop\n",
    "def process_dataset(model, image_paths, resize_to, batch_size):\n",
    "    total_load, total_inf, total_write = 0, 0, 0\n",
    "    batch_imgs, batch_paths, batch_src = [], [], []\n",
    "\n",
    "    for (path, src, trans, load_t) in prefetch_loader(image_paths, load_image, resize_to=resize_to, max_workers=4):\n",
    "        if isinstance(src, Exception):\n",
    "            print(f\"‚ùå Error loading {path}: {src}\")\n",
    "            continue\n",
    "        total_load += load_t\n",
    "\n",
    "        batch_paths.append(path)\n",
    "        batch_imgs.append(trans)\n",
    "        batch_src.append(src)\n",
    "\n",
    "        if len(batch_imgs) >= batch_size:\n",
    "            tensors = torch.stack(batch_imgs).to(DEVICE)\n",
    "            # outputs, inf_t = run_inference(model, device, tensors)\n",
    "            outputs = run_inference(model, batch_imgs, batch_paths, batch_src)\n",
    "            total_inf += outputs[3]\n",
    "\n",
    "            for p, s, o in zip(batch_paths, batch_src, [outputs]):\n",
    "                t0 = time.time()\n",
    "                annotate_and_save(p, s, o)\n",
    "                total_write += time.time() - t0\n",
    "\n",
    "            batch_imgs, batch_paths, batch_src = [], [], []\n",
    "\n",
    "    return total_load, total_inf, total_write\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(f\"üöÄ Using device: {DEVICE}\")\n",
    "    # ---------- Load model ----------\n",
    "    model = load_model(config_path, weights_path, device=DEVICE)\n",
    "\n",
    "\n",
    "    \n",
    "    image_paths = sorted(glob.glob(os.path.join(image_folder, \"*.*\")))[40000:]\n",
    "\n",
    "    # with open(val_txt, 'r') as file_object:\n",
    "    #     image_paths = [\n",
    "    #         os.path.join(image_folder, os.path.basename(x.strip()))\n",
    "    #         for x in file_object.readlines()\n",
    "    #         if \"whales\" in x\n",
    "    #     ]\n",
    "    \n",
    "\n",
    "    \n",
    "    print(f\"üì∑ Found {len(image_paths)} images\")\n",
    "\n",
    "    # --- output ---\n",
    "    os.makedirs(os.path.join(OUTPUT_FOLDER, \"images\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(OUTPUT_FOLDER, \"labels\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(OUTPUT_FOLDER, \"annotated\"), exist_ok=True)\n",
    "\n",
    "    # --- process ---\n",
    "    t0 = time.time()\n",
    "\n",
    "    for i in range(0, len(image_paths), 100):\n",
    "        t1 = time.time()\n",
    "        images = image_paths[i:i + 100]\n",
    "        load_t, inf_t, write_t = process_dataset(model, images, resize_to=RESIZE_TO, batch_size=1)\n",
    "        batch_time = time.time() - t1\n",
    "        total_time = time.time() - t0\n",
    "        print(f\" 100 images from {i} to {i + 100} finished, ‚è± Batch time: {batch_time:.2f}s; Total time: {total_time:.2f}s\")\n",
    "\n",
    "    # --- report ---\n",
    "    # print(f\"\\n‚úÖ Finished processing {len(image_paths)} images\")\n",
    "    # print(f\"‚è± Total time: {total_time:.2f}s\")\n",
    "    # print(f\"   - Loading:   {load_t:.2f}s\")\n",
    "    # print(f\"   - Inference: {inf_t:.2f}s\")\n",
    "    # print(f\"   - Writing:   {write_t:.2f}s\")\n",
    "    # print(f\"   - Avg/img:   {total_time/len(image_paths):.3f}s\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3f4585-7118-4072-969b-172d1f791f2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
